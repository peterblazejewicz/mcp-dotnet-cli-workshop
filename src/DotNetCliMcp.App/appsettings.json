// Notes:
// - Comments and trailing commas are supported by Microsoft.Extensions.Configuration.Json
// - Environment variable overrides use double underscore separators, e.g. OpenAI__Endpoint
// - The OpenAI endpoint should include /v1 to match OpenAI-compatible API structure (LM Studio)
{
  "OpenAI": {
    // Example: "http://127.0.0.1:1234/v1" for LM Studio local server
    "Endpoint": "http://127.0.0.1:1234/v1",
    // Default model name for LM Studio (adjust as needed)
    "Model": "local-model",
    // LM Studio typically doesn't require an API key; set a placeholder or real key as needed
    "ApiKey": "not-needed",
    // Request timeout for LLM calls (seconds). Use 0 to disable timeout (infinite).
    // Env override: OpenAI__HttpTimeoutSeconds
    "HttpTimeoutSeconds": 300,
    // TCP connect timeout for establishing a connection to LM Studio (seconds)
    // Env override: OpenAI__ConnectTimeoutSeconds
    "ConnectTimeoutSeconds": 15,
    // Generation controls (can be tuned per model)
    // Env override: OpenAI__Temperature
    // Lowered to 0.1 for DeepSeek R1 models to reduce reasoning loops and increase determinism
    "Temperature": 0.1,
    // Env override: OpenAI__MaxTokens
    "MaxTokens": 1500
  },
  "Logging": {
    // Minimum level used by Serilog configuration
    "MinimumLevel": "Information",
    "Console": {
      // Serilog console output template
      "OutputTemplate": "[{Timestamp:HH:mm:ss} {Level:u3}] {Message:lj}{NewLine}{Exception}"
    },
    "File": {
      // Rolling log file path (directory will be created if needed)
      "Path": "logs/mcp-dotnet-cli-workshop-.log"
    }
  }
}
